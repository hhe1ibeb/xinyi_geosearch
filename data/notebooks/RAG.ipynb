{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets pandas pymongo sentence_transformers\n",
    "%pip install -U transformers\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"hhe1ibeb/xinyi_geodata\")\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "dataset_df = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Remove data point where plot coloumn is missing\n",
    "dataset_df = dataset_df.dropna(subset=\"description\")\n",
    "print(\"\\nNumber of missing values in each column after removal:\")\n",
    "print(dataset_df.isnull().sum())\n",
    "\n",
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# https://huggingface.co/thenlper/gte-large\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "\n",
    "def get_embedding_en(text: str) -> list[float]:\n",
    "    if not text.strip():\n",
    "        print(\"Attempted to get embedding for empty text.\")\n",
    "        return []\n",
    "\n",
    "    embedding = embedding_model.encode(text)\n",
    "\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"embedding-en\"] = dataset_df[\"description\"].apply(get_embedding_en)\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# https://huggingface.co/thenlper/gte-large-zh\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large-zh\")\n",
    "\n",
    "\n",
    "def get_embedding_zh(text: str) -> list[float]:\n",
    "    if not text.strip():\n",
    "        print(\"Attempted to get embedding for empty text.\")\n",
    "        return []\n",
    "\n",
    "    embedding = embedding_model.encode(text)\n",
    "\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"embedding-zh\"] = dataset_df[\"descriptions-mandarin\"].apply(get_embedding_zh)\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MONGO_URI=mongodb+srv://hhe1ibeb:idbG7LqUV1ZButg9@xinyigeosearch.mlyr8or.mongodb.net/?retryWrites=true&w=majority&appName=XinyiGeoSearch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import os\n",
    "\n",
    "def get_mongo_client(mongo_uri):\n",
    "    \"\"\"Establish connection to the MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        print(\"Connection to MongoDB successful\")\n",
    "        return client\n",
    "    except pymongo.errors.ConnectionFailure as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "mongo_uri = os.getenv(\"MONGO_URI\")\n",
    "if not mongo_uri:\n",
    "    print(\"MONGO_URI not set in environment variables\")\n",
    "\n",
    "mongo_client = get_mongo_client(mongo_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data into MongoDB\n",
    "db = mongo_client[\"xinyi_geodata\"]\n",
    "collection = db[\"collection_1\"]\n",
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dataset_df.to_dict(\"records\")\n",
    "collection.insert_many(documents)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_en(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding_en(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding-en\",\n",
    "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "                \"limit\": 4,  # Return top 4 matches\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the _id field\n",
    "                \"lat\": 1,  # Include the lat field\n",
    "                \"lon\": 1,  # Include the lon field\n",
    "                \"description\": 1,  # Include the description field\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_result(query, collection):\n",
    "\n",
    "    get_knowledge = vector_search_en(query, collection)\n",
    "\n",
    "    search_result = \"\"\n",
    "    for result in get_knowledge:\n",
    "        search_result += f\"Lat: {result.get('lat', 'N/A')}, Lon: {result.get('lon', 'N/A')}, Description: {result.get('description', 'N/A')}\\n\"\n",
    "\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct query with retrival of sources\n",
    "query = \"I want to live in a quiet place with a lot of greenary\"\n",
    "source_information = get_search_result(query, collection)\n",
    "combined_information = (\n",
    "    f\"DOCUMENT:\\n{source_information}\\n\"\n",
    "    f\"QUESTION:\\n{query}.\\n\"\n",
    "    \"INSTRUCTION:\\n\"\n",
    "    \"You are a real estate agent whose job is to suggest the best place to the buyer. \"\n",
    "    \"You must explain and support your answer with the provided information. \"\n",
    "    \"Answer the user's QUESTION using the DOCUMENT text above. \"\n",
    "    \"Keep your answer grounded in the facts of the DOCUMENT. \"\n",
    "    \"If the DOCUMENT doesn’t contain the facts to answer the QUESTION, return {NONE}.\"\n",
    ")\n",
    "\n",
    "print(combined_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_zh(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding_zh(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index_zh\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding-zh\",\n",
    "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "                \"limit\": 4,  # Return top 4 matches\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the _id field\n",
    "                \"lat\": 1,  # Include the lat field\n",
    "                \"lon\": 1,  # Include the lon field\n",
    "                \"descriptions-mandarin\": 1,  # Include the description field\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_result_zh(query, collection):\n",
    "\n",
    "    get_knowledge = vector_search_zh(query, collection)\n",
    "\n",
    "    search_result = \"\"\n",
    "    for result in get_knowledge:\n",
    "        search_result += f\"Lat: {result.get('lat', 'N/A')}, Lon: {result.get('lon', 'N/A')}, Description: {result.get('descriptions-mandarin', 'N/A')}\\n\"\n",
    "\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conduct query with retrival of sources\n",
    "query = \"我想要找一個商店很多的地方\"\n",
    "source_information = get_search_result_zh(query, collection)\n",
    "combined_information = (\n",
    "    f\"DOCUMENT:\\n{source_information}\\n\"\n",
    "    f\"QUESTION:\\n{query}.\\n\"\n",
    "    \"INSTRUCTION:\\n\"\n",
    "    \"You are a real estate agent whose job is to suggest the best place to the buyer. \"\n",
    "    \"You must explain and support your answer with the provided information. \"\n",
    "    \"Answer the user's QUESTION using the DOCUMENT text above. \"\n",
    "    \"Keep your answer grounded in the facts of the DOCUMENT. \"\n",
    "    \"If the DOCUMENT doesn’t contain the facts to answer the QUESTION, return {NONE}.\"\n",
    ")\n",
    "\n",
    "print(combined_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to GPU\n",
    "input_ids = tokenizer(combined_information, return_tensors=\"pt\").to('cuda')\n",
    "response = model.generate(**input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
